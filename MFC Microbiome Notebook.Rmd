---
title: "MFC Microbiome Notebook"
output:
  html_notebook: default
  html_document: default
  pdf_document: default
---
This is code for my dissertation and eventual publication of this chapter as a research article. Extensively adapted from Josh Claypool's microbiome package.


Clear RStudio
```{r}

  # cat(rep("\n",50)) # Clear Console
  rm(list=ls(all=TRUE)) # clear workspace
  graphics.off() # closes all graphics
  setwd("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15") # sets working directory
```

Install function for needed packages
```{r, message = FALSE, warning=FALSE}
   
  packages<-function(x){
    x<-as.character(match.call()[[2]])
    if (!require(x,character.only=TRUE)){
      install.packages(pkgs=x,repos="http://cran.r-project.org")
      require(x,character.only=TRUE)
    }
  }
  packages(devtools) # List additional needed packages here on new lines with "packages(NAME)"
  packages(xlsx)
  packages(survival) # dependency for microbiome package
  packages(Formula) # dependency for microbiome package
  packages(ggplot2) # dependency for microbiome package
  packages(acepack) # dependency for microbiome package
  packages(base64enc) # dependency for microbiome package
  packages(gridExtra) # dependency for microbiome package
  packages(htmlTable) # dependency for microbiome package
  packages(data.table) # dependency for microbiome package
  packages(permute) # dependency for microbiome package
  packages(ggthemes)
  packages(GGally)
  source("https://bioconductor.org/biocLite.R")
  # biocLite("WGCNA")
  # biocLite("preprocessCore")
  install_github("jtclaypool/microbiome")
    #missing dependencies may need to be installed from Bioconductor
    #install missing packages by (for preprocessCore):
    #source("https://bioconductor.org/biocLite.R")
    #biocLite("preprocessCore")
  require(microbiome)
```

Read in Biom (tab-delimited) file
```{r}
# if file has not been read into R (filepath can be put in directly, "C://users/jtclaypool/Desktop/fir_data.txt"; or using the file.choose() command)
    biom=read.biom("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/mfc_otu_table_frd_exp15.txt")
  
  # you will also need your metadata file
    meta=read.table("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/metadata_frd_exp15.txt",header=T,sep="\t")
```

Fix Metada
```{r}
# This section fixes the metadata to match between the metadata labels and the R biome labels
  # Note that R puts an X in front of any column header that starts with a number, and dashes and other special characters are replaced by periods
  meta$Label = paste("X",meta$Label,sep = "")
  meta$Label = gsub("-",".", meta$Label)
```

Inspect data and write excel file
```{r}
 # # Inspect data
 # write.xlsx2(meta, file = "mybiome.xlsx", sheetName = "Metadata", append = FALSE) # Creates a new workbook
 # write.xlsx2(biom$RA.Otus, file = "mybiome.xlsx", sheetName = "RA.Otus", append = TRUE) # Adds a sheet to the existing workbook
 # write.xlsx2(biom$taxon, file = "mybiome.xlsx", sheetName = "taxon", append = TRUE) # Adds a sheet to the existing workbook
 # write.xlsx2(biom$biom_tab, file = "mybiome.xlsx", sheetName = "biom_tab", append = TRUE) # Adds a sheet to the existing workbook
 # shell.exec("mybiome.xlsx") # Opens the file
```

Create rarefaction curves for data
```{r}
# https://rdrr.io/rforge/vegan/man/rarefy.html
packages(vegan)
S <- specnumber(biom$RA.Otus) # observed number of species

# Modify data so that rows are samples, columns are reads per OTU
rare.data <- biom$biom_tab[,1:18] # leave off taxonomy on biom_tab
t.rare.data <- transpose(rare.data) # transpose https://stackoverflow.com/questions/6778908/r-transposing-a-data-frame
# get row and colnames in order
  colnames(t.rare.data) <- rownames(rare.data)
  rownames(t.rare.data) <- colnames(rare.data)

(raremax <- min(rowSums(t.rare.data)))
Srare <- rarefy(t.rare.data, raremax)
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
rarecurve(t.rare.data, step = 20, sample = raremax, col = "blue", cex = 0.6)
```

Simple Relative Abundance
```{r}
  # Here we can make our generic barplot of relative abundance. We can also then transform it into something fancier!
    ra_plot=barplot_RA(biom$RA.Otus,tax = biom$taxon,meta = meta,category = "Sample",top = 5)
    
  # plot is stored in list variable RA_plot
  # RA_plot is a ggplot2 object and can be manipulated according to make publication ready graph
  # for example adding an x-axis label and changing the legend title
  
  ggplot(data = ra_plot$top_long,aes(x=factor(label,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")),y=value,fill=variable))+ # this reorders x     axis values
    geom_bar(stat="identity")+
    scale_x_discrete("Sample")+ # Labels categorical labels on x axis
    ylab("Relative Abundance")+ 
    theme(legend.title=element_text(),legend.position="right",plot.title = element_text(hjust=0.5))+
    guides(fill=guide_legend("Phylum"))+
    theme_few()
```

Export Relative Abundance Data
```{r}
  ## If we don't want to make this graph in R but want to save ourselves some time, we can export the relative abundance data for use in a spreadsheet program.
  
  ## this will write it to your current working directory. The name in quotations will be the final name of the file
  # write.table(ra_plot$top_wide,"RA table.txt", sep="\t",row.names = T)
  # shell.exec("RA table.txt")
```

NMDS Plot
```{r}
veg=vegan_wrapper(biom$RA.Otus,meta = meta,category_1 = "Sample")
  
#again the ggplot2 graph can be edited
# Original Code
  # veg$NMDS_plot+
  #   theme(legend.title=element_text(),legend.position="right", plot.title = element_text(hjust=0.5))+
  #   guides(fill=guide_legend("Sample"))+
  #   theme_few()+
  #   aes(color=as.factor(meta[,category_1]))
  # veg$NMDS_plot

  
# Pull in sample type
NMDS.data <- veg$NMDS_plot$data
NMDS.data$Sample <- meta$Sample[match(rownames(NMDS.data),meta$Label)]
NMDS.data$Sample <- as.factor(NMDS.data$Sample)

# getting the convex hull of each unique point set
# https://stats.stackexchange.com/questions/22805/how-to-draw-neat-polygons-around-scatterplot-regions-in-ggplot2
packages(plyr)
find_hull <- function(df) df[chull(df$MDS1, df$MDS2), ]
hulls <- ddply(NMDS.data,.(Sample), find_hull)



 
 # https://gist.github.com/rmaia/5296401
 ggplot(data=NMDS.data, aes(x=MDS1, y=MDS2, color=Sample)) + geom_point(aes(shape=Sample),size=3)+
  scale_shape_manual(values=0:4) +
  geom_polygon(data=hulls, aes(x=MDS1, y=MDS2, fill=Sample), alpha=0.2) +
  scale_color_brewer(palette='Set1') +
  scale_fill_brewer(palette='Set1')+
  theme_few()
 
```

ANOVA on Shannon's Index
```{r}
# This is code to do ANOVA on Shannon's Index
  div = diversity(biom$RA.Otus, index = "shannon")
  div = data.frame(div)
  div = cbind(rownames(div),div)
  rownames(div) = NULL
  div_merge <- merge(x=div,y=meta, by.x = "rownames(div)", by.y = "Label")
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() options
  ggplot(div_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = div)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Shannon's Diversity Index (H)") +
    theme_few()
  
  
  # Export to CSV
  write.table(div_merge,"Diversity table.txt", sep="\t",row.names = F)
  
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(div ~ Sample, data = div_merge)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_diversity = aov(div~Sample,data=div_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_diversity)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_diversity,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results ----
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups ----
  cat("\n")
  print(TukeyHSD(aov_diversity))
  
```

That previous diversity data was based on the total OTU table, so it included multiple species within a single genus as contributing to increased diversity.
What happens if we group by genera in Excel first, then rerun this analysis?
```{r}
# This is code to do ANOVA on Shannon's Index
packages(xlsx)
pivot.data <- read.xlsx("genus_pivot_table.xlsx",2,as.data.frame = TRUE,header=TRUE)
rownames(pivot.data) <- pivot.data[,1]
pivot.data <- pivot.data[,-1]
# t.pivot.data <- t(pivot.data)
# colnames(t.pivot.data) <- t.pivot.data[1,]
# t.pivot.data <- t.pivot.data[-1,]
# t.pivot.data <- as.data.frame(t.pivot.data)
rownames(pivot.data) <- gsub("Sum of ", "", rownames(pivot.data))

  
  
div = diversity(pivot.data, index = "shannon")
  div = data.frame(div)
  div = cbind(rownames(div),div)
  rownames(div) = NULL
  

  
  div_merge <- merge(x=div,y=meta, by.x = "rownames(div)", by.y = "Label")
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() options
  ggplot(div_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = div)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Shannon's Diversity Index (H)") +
    theme_few()
  
  # Export to CSV
  # write.table(div_merge,"Diversity table.txt", sep="\t",row.names = F)
  
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(div ~ Sample, data = div_merge)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_diversity = aov(div~Sample,data=div_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_diversity)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_diversity,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results ----
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups ----
  cat("\n")
  print(TukeyHSD(aov_diversity))
  
```


```{r}
# This is code to do ANOVA on Species Richness (S)
  S = specnumber(biom$RA.Otus) # get species richness per sample analyzed
  S = data.frame(S) # convert to data frame
  S = cbind(rownames(S),S)
  rownames(S) = NULL
  S_merge <- merge(x=S,y=div_merge, by.x = "rownames(S)", by.y = "rownames(div)") # merge the metadata and species richness by row name
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() options
  ggplot(S_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = S)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Species Richness (S)") +
    theme_few()
  
  
  # Export to CSV
  write.table(S_merge,"Species richness table.txt", sep="\t",row.names = F)
  
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(S ~ Sample, data = S_merge)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_S = aov(S~Sample,data=S_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_S)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_S,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results ----
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups ----
  cat("\n")
  print(TukeyHSD(aov_S))
  
```
What if we try this again using our OTUs grouped by genera?
```{r}
# This is code to do ANOVA on Species Richness (S)
  S = specnumber(pivot.data) # get species richness per sample analyzed
  S = data.frame(S) # convert to data frame
  S = cbind(rownames(S),S)
  rownames(S) = NULL
  S_merge <- merge(x=S,y=div_merge, by.x = "rownames(S)", by.y = "rownames(div)") # merge the metadata and species richness by row name
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() options
  ggplot(S_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = S)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Species Richness (S)") +
    theme_few()
  
  
  # Export to CSV
  write.table(S_merge,"Species richness table.txt", sep="\t",row.names = F)
  
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(S ~ Sample, data = S_merge)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_S = aov(S~Sample,data=S_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_S)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_S,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results ----
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups ----
  cat("\n")
  print(TukeyHSD(aov_S))
  
```

```{r}
# This is code to do ANOVA on Pielou's Evenness (J)
  S_merge$J <- S_merge$div/log(S_merge$S)  # this is the definition of Pielou's Evenness. Note that log() is natural log.
  
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() optionJ
  ggplot(S_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = J)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Pielou's Evenness (J)") +
    theme_few()
  
  
  # Export to CSV
  write.table(S_merge,"pielous evenness table.txt", sep="\t",row.names = F)
  
  
# Fit a model uJing the lm function and look at the parameter eJtimateJ and Jtandard errorJ for the treatment effectJ
  # https://www.r-bloggerJ.com/one-way-analyJiJ-of-variance-anova/
  model = lm(J ~ Sample, data = S_merge)
  # We Jave the model fitted to the data in an object Jo that we can undertake variouJ actionJ to Jtudy the goodneJJ of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_J = aov(J~Sample,data=S_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_J)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_J,"means"),digitJ=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results ----
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # ExtractJ firJt row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups ----
  cat("\n")
  print(TukeyHSD(aov_J))
  
```


Why are the bristles still showing higher diversity than our sludge inoculum?
```{r, message = FALSE, warning=FALSE}
# Aggregate OTUs by sample
otus_by_sample <- biom$biom_tab
otus_by_sample <- t(otus_by_sample)

# Remove phylogeny info
otus_by_sample <- otus_by_sample[-c(19:25), ]

# Look up sample names
otus_by_sample <- as.data.frame(otus_by_sample)
otus_by_sample$Sample <- rownames(otus_by_sample)
otus_by_sample <- otus_by_sample[,c(ncol(otus_by_sample),1:(ncol(otus_by_sample)-1))]
rownames(otus_by_sample) <- NULL
otus_by_sample$Sample <- meta[match(otus_by_sample$Sample,meta$Label),"Sample"]


# aggregate by sample type (https://stackoverflow.com/questions/1660124/how-to-sum-a-variable-by-group)
otu_sample_names <- otus_by_sample$Sample
indx <- sapply(otus_by_sample, is.factor)
otus_by_sample[indx] <- lapply(otus_by_sample[indx], function(x) as.numeric(as.character(x)))
otus_by_sample$Sample <- otu_sample_names

otus_agg_by_sample <- aggregate(. ~ Sample,otus_by_sample, "sum")


# Transform
otus_agg_by_sample <- t(otus_agg_by_sample)
colnames(otus_agg_by_sample) <- otus_agg_by_sample[1,]
otus_agg_by_sample <- otus_agg_by_sample[-1,]

# Filter OTUs by in bristles, not in sludge
otus_agg_by_sample <- as.data.frame(otus_agg_by_sample)
sludge_vs_bristles <- otus_agg_by_sample[otus_agg_by_sample$Sludge == 0 & otus_agg_by_sample$Bristles != 0,]
# sludge_vs_bristles[,-c(2,3,4)]

n_bristle_otus <- nrow(sludge_vs_bristles)
# Make histogram of bristle reads
ggplot(data=sludge_vs_bristles, aes(sludge_vs_bristles$Bristles)) + 
    geom_bar(col="black",fill="grey") +
    xlab("No. reads per OTU")+
    annotate("text",x=5,y=70,label=paste(n_bristle_otus, "OTUs found in bristles and not in sludge"))+
    theme_few()


```
Let's compare this data to the original biom file that didn't have singletons removed. How many of these OTUs came from singletons in the sludge, and how many did not show up at all in the sludge? Then let's make a bar chart of those two counts.
```{r Bristle OTU Reads from original biom file}
# Can't read in original file with read.biom because read.biom removes singletons then filters for only OTUs that had reads in my samples
    orig_biom=read.table("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/PICRUST/Exp15-microbiom COPY/otu_table.txt",header=T,sep="\t")

head(orig_biom)

# Sum bristle samples

meta$Label[c(which(meta$Sample=="Sludge"))] # Return label names which match sample "Sludge"
orig_biom$Orig_Sludge <- rowSums(orig_biom[,(meta$Label[c(which(meta$Sample=="Sludge"))])]) # Create a new summed column in the original biom file

# Match original sludge reads into new sludge vs bristles table by looking up OTU
sludge_vs_bristles$Orig_Sludge <- orig_biom$Orig_Sludge[match(rownames(sludge_vs_bristles),orig_biom$OTU.ID)]

# Plot
# Make histogram of read
ggplot(data=sludge_vs_bristles, aes(as.factor(sludge_vs_bristles$Orig_Sludge))) + 
    geom_bar(col="black",fill="grey") +
    xlab("No. reads in sludge per OTU without singleton removal")+
    geom_text(stat='count',aes(label=..count..),vjust=-1)+
  scale_y_continuous(limits=c(0,200))+
  theme_few()
```


These relationships are interesting. Many of the relationships look similar. Let's do a scatterplot matrix to see what these relationships look like.
```{r Scatterplot of ecological metrics}
# https://stackoverflow.com/questions/3735286/create-a-matrix-of-scatterplots-pairs-equivalent-in-ggplot2
ggpairs(S_merge, columns = c(2,3,9), aes(colour = Sample, alpha = 0.4))+theme_few()
```

Now that we've explored our ecological metrics, let's dig into some of the network analysis of our microbial communities.

# Microbial Community Networks

 So this is the basic works, but getting into some of the network development, what we are trying to extract is how our microbial community is interacting.
 There are several ways to produce co-occurrence networks.

 Pearson Correlations - generate linear relationship between OTU's. This means organisms have to increase and decrease at the same rate to become captured in the network. This can be of relative abundance, raw counts, or transformed data (generally centered-log-transformed)
 Spearman Correlations - generate rank-based relationships between OTU's. This allows data to increase or decrease together but isn't restricted to linear relationships. This again can be relative abundance, raw counts, or transformed data (generally centered-log-transformed).
 Other novel programs exist but will be outside the scope of this package
 SparCC
 Spiec-Easi (Speak-easy)
 Some people will use rarefied counts but this removes data and recent studies suggest NOT to do this.
 So here, we will use relative abundance to just get familiar with the ideas until a centered-log-ratio is implemented. The reason relative abundance isn't great for networks is that OTU's with large counts produce false correlations due to their predominance.

# Co-occurrence -----------------------------------------------------
```{r, message = FALSE, warning=FALSE}
  
  # This is an area of networks where we study OTU's in the same environment that "co-occur". First we will filter data by co-occurence. Minimum recommended is 20%.
  
  #filter OTU's to 50% presence in all samples
  biom_fil=cooccur_filter(RA=biom$RA.Otus,co_per=0.5)
  
  #run co-occurence. Taxon can be excluded and identified later if desired.
  biom_netw=cooccurrence(biom_fil,taxon = biom$taxon)
  
  #try plotting the data
  # plot.igraph(biom_netw$netw) # standard plot function for igraph package
  # tkplot(biom_netw$netw) #interactive plot viewer
  # rglplot(biom_netw$netw) #experimental 3d plot viewer
  
  #Or by using ggnet2 (https://briatte.github.io/ggnet/)
  packages(network)
  packages(sna)
  packages(RColorBrewer)
  packages(intergraph)
  
  # Visualize
  set.seed(101)
  ggnet2(biom_netw$netw, size = 6, color = "tomato")
  
```
What if we run the same code but only on the bristle samples?
```{r Networks in Bristle Samples}
  
  # This is an area of networks where we study OTU's in the same environment that "co-occur". First we will filter data by co-occurence. Minimum recommended is 20%.
  
# Filter biom$RA.Otus to just bristle samples
biom$RA.Otus.bristles <- biom$RA.Otus[meta[meta$Sample=="Bristles","Label"],]
  
#filter OTU's to 50% presence in all samples
  biom_fil_bristles=cooccur_filter(RA=biom$RA.Otus.bristles,co_per=0.5)
  
  #run co-occurence. Taxon can be excluded and identified later if desired. Making a new function here since rcorr doesn't work with n<5
 cooccurrence2 <- function (data = "relative abundance", taxon = NULL, cor = 0.6, pval = 0.01) 
{
  require(igraph)
  # require(Hmisc)
  require(psych)
 
  corrMatrix <- corr.test(as.matrix(data),adjust="none",ci=F,method="pearson")
  pAdjusted = p.adjust(corrMatrix$p, method = "BH")
  corrMatrixMin = (((corrMatrix$r > cor) * 1 + (pAdjusted < pval) * 1) == 2) * 1
  diag(corrMatrixMin) = 0
  corrMatrixTax = corrMatrixMin[rowSums(corrMatrixMin) > 1, colSums(corrMatrixMin) > 1]
  netw.corr = graph.adjacency(corrMatrixMin, mode = "undirected", weighted = TRUE)
  netw.corr.trim = delete_vertices(netw.corr, igraph::degree(netw.corr) < 1)
  ifelse(is.null(taxon), taxon.netw <- NULL, taxon.netw <- taxon[which(taxon[, 
    1] %in% V(netw.corr.trim)$name), ])
  return(list(corr = corrMatrix, corrMin = corrMatrixMin, 
    netw = netw.corr.trim, taxon.netw = taxon.netw, pAdjusted = as.matrix(pAdjusted)))
}
  biom_netw_bristles=cooccurrence2(biom_fil_bristles,taxon = biom$taxon)
  
  #try plotting the data
  # plot.igraph(biom_netw$netw) # standard plot function for igraph package
  # tkplot(biom_netw$netw) #interactive plot viewer
  # rglplot(biom_netw$netw) #experimental 3d plot viewer
  
  #Or by using ggnet2 (https://briatte.github.io/ggnet/)
  packages(network)
  packages(sna)
  packages(RColorBrewer)
  packages(intergraph)
  
  # Visualize
  set.seed(101)
  (network_plot_bristles <- ggnet2(biom_netw_bristles$netw, size = 6, color = "tomato"))
```


# Community Detection -----------------------------------------------------

  # Now once you've seen your network, you are probably noticing clusters together and maybe that's interesting to you. If so, community detection is next up. This will allow you to find interacting groups of OTU's that may be functioning together in your samples and worth investigating more thoroughly. 
  # As always multiple methods exist and most can be found within the igraph package of R. Two review papers comparing community detection algorithms can be found here and hereTo list a few algorithms:
  # 
  # fastgreedy *greedy modularity maximisation
  # infomap *information compression
  # louvain *multilevel modularity maximization
  # walktrap
  # uses small random walks to identify most likely neighbors
  # walktrap (modularity optimized - this package)
  # same as walktrap but optimizes based on modularity
  # Each one of these algorithms can utilize the network you just created to detect these community niches.
  
  #infomap community detection. Try the different detection algorithms to understand how different your niches might be broken up
```{r}
biom_info=infomap.community(biom_netw$netw)

#now add some color to your previous plot (Josh's Method)
  # plot(biom_netw$netw,vertex.color=as.factor(biom_info$membership))
  
#Alternatively
set.seed(101)
plot_ggnet <- ggnet2(biom_netw$netw, size = 6, color = as.factor(biom_info$membership), palette = "Paired", color.legend = "Cluster")
plot_ggnet
  #and to understand how these subcommunities are present in your overall community
  #this may show a warning message if the community modules/clusters exceed 13. This is just because of lacking a distinct palette color for each cluster. It may also be harder to interpret yourself. You can plot anyway with the following line.
  plot_module=barplot_module(data=biom$RA.Otus,niche = biom_info,meta = meta, categories = "Sample")
  plot_module$plot+
    scale_x_discrete("Sample")+ # Labels categorical labels on x axis
    ylab("Relative Abundance (%)")+ 
    theme(legend.title=element_text(),legend.position="right",plot.title = element_text(hjust=0.5))+
    theme_few()
```  
  
# Keystone Microbes -------------------------------------------------------
  
Next we'll try and find some keystone microbes (if there are some!). This is largely built on heuristics from modularity of bee pollination networks. Nevertheless it is the gold-standard at the moment. Keystone are identified as either:
 Hub: forms a dense set of network connections within its own module such that the disappearance of such OTU may signify large changes for module structure or module collapse
  Connector: Is largely connected to many different modules bringing together many different microbial niches. The disappearance of these OTU's may remove the ability of the niches to function together in the same environment
  
  This is passed our network information and module membership information. Because of an iterative process, this can sometimes take a little bit to work
```{r}
 biom_zipi=ZiPi(biom_netw$netw,modules=biom_info$membership)
```  
  
 
  
  this can be visualized as such. Sorry I haven't put a code for this yet but you get the table to plot with it as you will or export it to excel if you desire. 

```{r}
par(xpd=FALSE)
  plot(biom_zipi$P,biom_zipi$Z,
       ylim = c(-3.5,3),
       ylab="Zi",
       xlab="Pi")
  #connectors are defined as having a Pi value > 0.62
  abline(v=0.62)
  #hubs are defined as having a Zi value > 2.5
  abline(h=2.5)
  #we will color any hubs or connectors red
  points(biom_zipi$P[biom_zipi$P>=0.62],biom_zipi$Z[biom_zipi$P>=0.62],col="red",pch=1) # these cutoff values are from literature
  points(biom_zipi$P[biom_zipi$Z>=2.5],biom_zipi$Z[biom_zipi$Z>=2.5],col="red",pch=1) # these cutoff values are from literature
  
  
  # Alternatively in ggplot
  zi.pi.ggplot <- ggplot(biom_zipi, aes(x = P, y = Z, color = ifelse(P>=.62|Z>=2.5,"red","green"))) #these colors don't seem to be working
  zi.pi.ggplot <- zi.pi.ggplot + geom_point(size = 4) + theme_few() + xlab("Pi")+ ylab("Zi")+
  geom_vline(xintercept = 0.62)+
  geom_hline(yintercept = 2.5)+
  theme(legend.position="none") #turns legend off
  zi.pi.ggplot
  
  #Let's return those OTUs of interest
  library(dplyr)
  biom_zipi_df <- as.data.frame(biom_zipi)
  filter(biom_zipi_df, P>=.62) # Connectors
  filter(biom_zipi_df, Z>=2.5) # Hubs
```


What sort of community network is this? Small world?

# KEGG Orthologs from PICRUST analysis
Here we want to answer three main questions. First, which types of genes were best enriched in each of our sample groups?
Second, were exoelectrogenic genes enriched in our bristles?
Third, which genes were enriched across each network cluster?

First, which types of genes were best enriched in each of our sample groups?
```{r, fig.width=9,fig.height=9}
# Let's try level 2 - it's got good granularity without too many different types of KOs
l2_picrust <- read.biom(
  "C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/PICRUST/Exp15-microbiom COPY/predicted_metagenome.L2.txt",
  new = T, metagenome = T)


# Align my data using label names in metadata
meta$Label
rownames(l2_picrust$RA.genes) # see if row names match up in same order
l2_picrust$RA.genes <- l2_picrust$RA.genes[match(meta$Label,rownames(l2_picrust$RA.genes)),] # make samples line up according to meta data so we can aggregate

# aggregate by sample type (https://stackoverflow.com/questions/1660124/how-to-sum-a-variable-by-group)
l2_agg = aggregate(l2_picrust$RA.genes,by=list(meta$Sample),FUN=mean)

# Then melt
melt_l2_agg <- melt(l2_agg,id="Group.1")

# Generate heat map
ggplot(data=melt_l2_agg,aes(x=Group.1,y=variable,fill=value))+
  geom_tile(color="white",size=.8)+ # puts padding in between colored cells
  scale_fill_gradient2("Gene relative abundance (%)\n",guide="colourbar",high="steelblue",low="white")+
  xlab("Sample")+
  ylab("KEGG Ortholog Group")+
  scale_x_discrete(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0, 0)) +
  theme_few()+
  theme(legend.title = element_text(hjust = 5))+
  ylim(rev(levels(melt_l2_agg$variable))) # Puts alphabetically from top

# What if we rescale and plot? https://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/
melt_l2_agg <- ddply(melt_l2_agg, .(variable), transform, rescale = scales::rescale(value)) # this rescales each value 0:1 by variable

(l2_heatmap1 <- ggplot(data=melt_l2_agg,aes(x=Group.1,y=variable,fill=rescale))+
  geom_tile(color="white",size=.8)+ # puts padding in between colored cells
  scale_fill_gradient2("Scaled gene relative abundance\n",guide="colourbar",high="steelblue",low="white")+
  xlab("Sample")+
  ylab("KEGG Ortholog Group")+
  scale_x_discrete(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0, 0)) +
  theme_few()+
  theme(legend.title = element_text(hjust = 5))+
  ylim(rev(levels(melt_l2_agg$variable)))) # Puts alphabetically from top
  # ggtitle("Gene abundance over time")

# What if we rescale and plot? https://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/
melt_l2_agg <- ddply(melt_l2_agg, .(variable), transform, scale = scale(value)) # this rescales each value 0:1 by variable

(l2_heatmap1 <- ggplot(data=melt_l2_agg,aes(x=Group.1,y=variable,fill=scale))+
  geom_tile(color="white",size=.8)+ # puts padding in between colored cells
  scale_fill_gradient2("Scaled gene relative abundance\n",guide="colourbar",high="steelblue",low="white")+
  xlab("Sample")+
  ylab("KEGG Ortholog Group")+
  scale_x_discrete(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0, 0)) +
  theme_few()+
  theme(legend.title = element_text(hjust = 5))+
  ylim(rev(levels(melt_l2_agg$variable)))) # Puts alphabetically from top
  # ggtitle("Gene abundance over time")
```
Second, were exoelectrogenic genes enriched in our bristles?
```{r}
# Get data from PICRUSt JSON file----------------------------------------------------------------
predicted_metagenome = read.biom("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/PICRUST/Exp15-microbiom COPY/PICRUST Metagenome USE THIS.tab",new=T,metagenome=T)
# predicted_metagenome$RA.genes[1:10,1:10] # View data # Note this gives relative abundance in percent from 0 to 100

# Modify data to align it-------------------------------------------------------------
# meta$Label # uses metadata from the fir data set of Josh's microbiome package
# rownames(predicted_metagenome$RA.genes) # see if row names match up in same order
predicted_metagenome$RA.genes <- predicted_metagenome$RA.genes[match(meta$Label,rownames(predicted_metagenome$RA.genes)),]
# rownames(predicted_metagenome$RA.genes)

# Aggregate genes by sample ----
genes_agg = aggregate(predicted_metagenome$RA.genes,by=list(meta$Sample),FUN=mean)


# Convert wide format into long format table ------------------------------
genes_melt=melt(genes_agg,id="Group.1")

# Filter by K01179 (Endoglucanase) and K05350 and K00430 (or whatever genes of interest)----

kos_exoelectrogenic <- read.csv("kos_exoelectrogenic.csv",header=FALSE,fileEncoding="UTF-8-BOM")


genes_filtered <- genes_melt[which(genes_melt$variable %in% unlist(kos_exoelectrogenic)),]


# Make heat map -----------------------------------------------------------

ggplot(data=genes_filtered,aes(x=Group.1,y=variable,fill=value))+
  geom_tile()+
  scale_fill_gradient2("Gene Relative Abundance",guide="colourbar",high="#FF5733",low="#FDFEFE",midpoint=mean(genes_filtered$value)-.02)+
  xlab("Timepoint")+
  ylab("KEGG Ortholog")+
  ggtitle("Gene abundance over time")

# Now what if we rescale the data?
genes_filtered <- ddply(genes_filtered, .(variable), transform, rescale = scales::rescale(value)) # this rescales each value 0:1 by variable

ggplot(data=genes_filtered,aes(x=Group.1,y=variable,fill=rescale))+
  geom_tile()+
  scale_fill_gradient2("Scaled gene Relative Abundance \n",guide="colourbar",high="#FF5733",low="#FDFEFE")+
  xlab("Sample")+
  ylab("KEGG Ortholog")

```

Third, which genes were enriched across each network cluster?
```{r}
# Aggregate KOs by network cluster

ptm <- proc.time()
ko_metagenome_contributions <- read.table("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/PICRUST/Exp15-microbiom COPY/ko_metagenome_contributions.tab",fill=TRUE)
proc.time() - ptm
```


# Yet to be done: Community Connects to the output ----------------------------------------
  # Here we will use ModuleEigengenes from the WGCNA package in R. WGCNA will require several installations from the Bioconductor site. This was developed to link genetic studies to diseases and conditions in the medical field. While WGCNA has their own community detection algorith embedded, this wrapper allows us to substitute our own and establish correlation between our detected communities and variables we've recored. The output is ready to be plotted with ggplot2.
  
  #first we call the function to establish the relationships. 
  biom_eigen=eigen_correlation(biom$RA.Otus[-c(5,8),],community = biom_info,metadata = meta[-(1:2),],categories = c("Xylanase.IU.g.dry.matter","Endoglucanase.IU..g.dry.matter","cCER"))
  
  #from there we can plot these correlations in a heatmap to visualize the relationship. 
  ggplot(data=biom_eigen$melt_cor,aes(x=as.factor(variable), y=category,fill=value))+
    geom_tile(colour="#B8B8B8")+
    #geom_text(aes(label=value))+
    scale_fill_gradient2("Degreee of \n Correlation",guide = "colourbar",high = "#7DEB5F",mid="#F0EE54",low="#F3633F",na.value="white",limits=c(-0.75,0.75))+ 
    ylab("")+
    xlab("Cluster/Module")+
    labs(fill="Cluster to Deconstruction")+
    scale_y_discrete(labels=c("Xylanase","Endoglucanase","cCER"))
  
  #we should probably filter on significance though
  ggplot(data=biom_eigen$melt_cor,aes(x=as.factor(variable), y=category,fill=ifelse(pval<=0.1,value,NA)))+
    geom_tile(colour="#B8B8B8")+
    #geom_text(aes(label=value))+
    scale_fill_gradient2("Degreee of \n Correlation",guide = "colourbar",high = "#7DEB5F",mid="#F0EE54",low="#F3633F",na.value="white",limits=c(-0.75,0.75))+ 
    ylab("")+
    xlab("Cluster/Module")+
    labs(fill="Cluster to Deconstruction")+
    scale_y_discrete(labels=c("Xylanase","Endoglucanase","cCER"))
  