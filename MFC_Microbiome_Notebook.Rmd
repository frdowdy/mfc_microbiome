---
title: "MFC Microbiome Notebook"
output:
  html_document:
    toc: yes
  html_notebook: default
  pdf_document: default
---
This is code for my dissertation and eventual publication of this chapter as a research article. Extensively adapted and shamelessly stolen from Josh Claypool's microbiome package (https://github.com/jtclaypool/microbiome).

First we're going to set up R to to our target directory and clear the workspace.
```{r setup}
knitr::opts_knit$set(root.dir = 'C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15')
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE, 
                      message = FALSE,
                      engine = "R", # Chunks will always have R code, unless noted
                      error = TRUE,
                      fig.path="C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/git_code/Figures/",  # Set the figure options
                      fig.align = "center",
                      dev='png')

```

```{r}
# Clear R studio
# cat(rep("\n",50)) # Clear Console
rm(list=ls(all=TRUE)) # clear workspace
graphics.off() # closes all graphics
```

Install function for needed packages
```{r, message = FALSE, warning=FALSE}
   
  packages<-function(x){
    x<-as.character(match.call()[[2]])
    if (!require(x,character.only=TRUE)){
      install.packages(pkgs=x,repos="http://cran.r-project.org",dependencies = TRUE)
      require(x,character.only=TRUE)
    }
  }
  packages(devtools) # List additional needed packages here on new lines with "packages(NAME)"
  packages(xlsx)
  packages(survival) # dependency for microbiome package
  packages(Formula) # dependency for microbiome package
  packages(ggplot2) # dependency for microbiome package
  packages(acepack) # dependency for microbiome package
  packages(base64enc) # dependency for microbiome package
  packages(gridExtra) # dependency for microbiome package
  packages(htmltools)
  packages(stringi)
  packages(htmlTable) # dependency for microbiome package
  packages(data.table) # dependency for microbiome package
  packages(permute) # dependency for microbiome package
  packages(ggthemes)
  packages(GGally)
  packages(survival)
  source("https://bioconductor.org/biocLite.R")
  # biocLite("WGCNA")
  # biocLite("preprocessCore")
  install_github("jtclaypool/microbiome")
    #missing dependencies may need to be installed from Bioconductor
    #install missing packages by (for preprocessCore):
    #source("https://bioconductor.org/biocLite.R")
    #biocLite("preprocessCore")
  require(microbiome)
```

Read in Biom (tab-delimited) file
```{r}
# if file has not been read into R (filepath can be put in directly, "C://users/jtclaypool/Desktop/fir_data.txt"; or using the file.choose() command)
    biom=read.biom("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/mfc_otu_table_frd_exp15.txt")
  
  # you will also need your metadata file
    meta=read.table("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/metadata_frd_exp15.txt",header=T,sep="\t")
```

Fix Metada
```{r}
# This section fixes the metadata to match between the metadata labels and the R biome labels
  # Note that R puts an X in front of any column header that starts with a number, and dashes and other special characters are replaced by periods
  meta$Label = paste("X",meta$Label,sep = "")
  meta$Label = gsub("-",".", meta$Label)
```

Inspect data and write excel file
```{r}
 # # Inspect data
 # write.xlsx2(meta, file = "mybiome.xlsx", sheetName = "Metadata", append = FALSE) # Creates a new workbook
 # write.xlsx2(biom$RA.Otus, file = "mybiome.xlsx", sheetName = "RA.Otus", append = TRUE) # Adds a sheet to the existing workbook
 # write.xlsx2(biom$taxon, file = "mybiome.xlsx", sheetName = "taxon", append = TRUE) # Adds a sheet to the existing workbook
 # write.xlsx2(biom$biom_tab, file = "mybiome.xlsx", sheetName = "biom_tab", append = TRUE) # Adds a sheet to the existing workbook
 # shell.exec("mybiome.xlsx") # Opens the file
```

Create rarefaction curves for data
```{r rarefaction curves}
# https://rdrr.io/rforge/vegan/man/rarefy.html
packages(vegan)
S <- specnumber(biom$RA.Otus) # observed number of species

# Modify data so that rows are samples, columns are reads per OTU
rare.data <- biom$biom_tab[,1:18] # leave off taxonomy on biom_tab
t.rare.data <- transpose(rare.data) # transpose https://stackoverflow.com/questions/6778908/r-transposing-a-data-frame
# get row and colnames in order
  colnames(t.rare.data) <- rownames(rare.data)
  rownames(t.rare.data) <- colnames(rare.data)

(raremax <- min(rowSums(t.rare.data)))
Srare <- rarefy(t.rare.data, raremax)
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
rarecurve(t.rare.data, step = 20, sample = raremax, col = "blue", cex = 0.6)
```

Simple Relative Abundance
```{r}
  # Here we can make our generic barplot of relative abundance. We can also then transform it into something fancier!
    ra_plot=barplot_RA(biom$RA.Otus,tax = biom$taxon,meta = meta,category = "Sample",top = 5)
    
  # plot is stored in list variable RA_plot
  # RA_plot is a ggplot2 object and can be manipulated according to make publication ready graph
  # for example adding an x-axis label and changing the legend title
  
  ggplot(data = ra_plot$top_long,aes(x=factor(label,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")),y=value,fill=variable))+ # this reorders x     axis values
    geom_bar(stat="identity")+
    scale_x_discrete("Sample")+ # Labels categorical labels on x axis
    ylab("Relative Abundance")+ 
    theme(legend.title=element_text(),legend.position="right",plot.title = element_text(hjust=0.5))+
    guides(fill=guide_legend("Phylum"))+
    theme_few()
```

Export Relative Abundance Data
```{r}
  ## If we don't want to make this graph in R but want to save ourselves some time, we can export the relative abundance data for use in a spreadsheet program.
  
  ## this will write it to your current working directory. The name in quotations will be the final name of the file
  # write.table(ra_plot$top_wide,"RA table.txt", sep="\t",row.names = T)
  # shell.exec("RA table.txt")
```

NMDS Plot
```{r NMDS from vegan}
veg=vegan_wrapper(biom$RA.Otus,meta = meta,category_1 = "Sample")
  
#again the ggplot2 graph can be edited
# Original Code
  # veg$NMDS_plot+
  #   theme(legend.title=element_text(),legend.position="right", plot.title = element_text(hjust=0.5))+
  #   guides(fill=guide_legend("Sample"))+
  #   theme_few()+
  #   aes(color=as.factor(meta[,category_1]))
  # veg$NMDS_plot

  
# Pull in sample type
NMDS.data <- veg$NMDS_plot$data
NMDS.data$Sample <- meta$Sample[match(rownames(NMDS.data),meta$Label)]
NMDS.data$Sample <- as.factor(NMDS.data$Sample)

# getting the convex hull of each unique point set
# https://stats.stackexchange.com/questions/22805/how-to-draw-neat-polygons-around-scatterplot-regions-in-ggplot2
packages(plyr)
find_hull <- function(df) df[chull(df$MDS1, df$MDS2), ]
hulls <- ddply(NMDS.data,.(Sample), find_hull)



 
 # https://gist.github.com/rmaia/5296401
 ggplot(data=NMDS.data, aes(x=MDS1, y=MDS2, color=Sample)) + geom_point(aes(shape=Sample),size=3)+
  scale_shape_manual(values=0:4) +
  geom_polygon(data=hulls, aes(x=MDS1, y=MDS2, fill=Sample), alpha=0.2) +
  scale_color_brewer(palette='Set1') +
  scale_fill_brewer(palette='Set1')+
  theme_few()
 
```

ANOVA on Shannon's Index
```{r ANOVA on Shannons Index}
# This is code to do ANOVA on Shannon's Index
  div = diversity(biom$RA.Otus, index = "shannon")
  div = data.frame(div)
  div = cbind(rownames(div),div)
  rownames(div) = NULL
  div_merge <- merge(x=div,y=meta, by.x = "rownames(div)", by.y = "Label")
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() options
  ggplot(div_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = div)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Shannon's Diversity Index (H)") +
    theme_few()
  
  
  # Export to CSV
  write.table(div_merge,"Diversity table.txt", sep="\t",row.names = F)
  
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(div ~ Sample, data = div_merge)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_diversity = aov(div~Sample,data=div_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_diversity)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_diversity,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results  
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups  
  cat("\n")
  print(TukeyHSD(aov_diversity))
  
```

That previous diversity data was based on the total OTU table, so it included multiple species within a single genus as contributing to increased diversity.
What happens if we group by genera in Excel first, then rerun this analysis?
```{r}
# This is code to do ANOVA on Shannon's Index
packages(xlsx)
pivot.data <- read.xlsx("genus_pivot_table.xlsx",2,as.data.frame = TRUE,header=TRUE)
rownames(pivot.data) <- pivot.data[,1]
pivot.data <- pivot.data[,-1]
# t.pivot.data <- t(pivot.data)
# colnames(t.pivot.data) <- t.pivot.data[1,]
# t.pivot.data <- t.pivot.data[-1,]
# t.pivot.data <- as.data.frame(t.pivot.data)
rownames(pivot.data) <- gsub("Sum of ", "", rownames(pivot.data))

  
  
div = diversity(pivot.data, index = "shannon")
  div = data.frame(div)
  div = cbind(rownames(div),div)
  rownames(div) = NULL
  

  
  div_merge <- merge(x=div,y=meta, by.x = "rownames(div)", by.y = "Label")
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() options
  ggplot(div_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = div)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Shannon's Diversity Index (H)") +
    theme_few()
  
  # Export to CSV
  # write.table(div_merge,"Diversity table.txt", sep="\t",row.names = F)
  
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(div ~ Sample, data = div_merge)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_diversity = aov(div~Sample,data=div_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_diversity)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_diversity,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results  
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups  
  cat("\n")
  print(TukeyHSD(aov_diversity))
  
```


```{r}
# This is code to do ANOVA on Species Richness (S)
  S = specnumber(biom$RA.Otus) # get species richness per sample analyzed
  S = data.frame(S) # convert to data frame
  S = cbind(rownames(S),S)
  rownames(S) = NULL
  S_merge <- merge(x=S,y=div_merge, by.x = "rownames(S)", by.y = "rownames(div)") # merge the metadata and species richness by row name
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() options
  ggplot(S_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = S)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Species Richness (S)") +
    theme_few()
  
  
  # Export to CSV
  write.table(S_merge,"Species richness table.txt", sep="\t",row.names = F)
  
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(S ~ Sample, data = S_merge)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_S = aov(S~Sample,data=S_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_S)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_S,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results  
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups  
  cat("\n")
  print(TukeyHSD(aov_S))
  
```
What if we try this again using our OTUs grouped by genera?
```{r}
# This is code to do ANOVA on Species Richness (S)
  S = specnumber(pivot.data) # get species richness per sample analyzed
  S = data.frame(S) # convert to data frame
  S = cbind(rownames(S),S)
  rownames(S) = NULL
  S_merge <- merge(x=S,y=div_merge, by.x = "rownames(S)", by.y = "rownames(div)") # merge the metadata and species richness by row name
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() options
  ggplot(S_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = S)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Species Richness (S)") +
    theme_few()
  
  
  # Export to CSV
  write.table(S_merge,"Species richness table.txt", sep="\t",row.names = F)
  
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(S ~ Sample, data = S_merge)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_S = aov(S~Sample,data=S_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_S)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_S,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results  
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups
  cat("\n")
  print(TukeyHSD(aov_S))
  
```

```{r}
# This is code to do ANOVA on Pielou's Evenness (J)
  S_merge$J <- S_merge$div/log(S_merge$S)  # this is the definition of Pielou's Evenness. Note that log() is natural log.
  
  # The geom_boxplot() option is used to specify background and outline colours for the boxes. 
  # The axis labels are created with the xlab() and ylab() optionJ
  ggplot(S_merge, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = J)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Pielou's Evenness (J)") +
    theme_few()
  
  
  # Export to CSV
  write.table(S_merge,"pielous evenness table.txt", sep="\t",row.names = F)
  
  
# Fit a model uJing the lm function and look at the parameter eJtimateJ and Jtandard errorJ for the treatment effectJ
  # https://www.r-bloggerJ.com/one-way-analyJiJ-of-variance-anova/
  model = lm(J ~ Sample, data = S_merge)
  # We Jave the model fitted to the data in an object Jo that we can undertake variouJ actionJ to Jtudy the goodneJJ of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_J = aov(J~Sample,data=S_merge)  #do the analysis of variance
  # cat("\n")
  # summary(aov_J)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_J,"means"),digitJ=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # ExtractJ firJt row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups  
  cat("\n")
  print(TukeyHSD(aov_J))
  
```


Why are the bristles still showing higher diversity than our sludge inoculum?
```{r OTUs in bristles vs sludge, message = FALSE, warning=FALSE}
# Aggregate OTUs by sample
otus_by_sample <- biom$biom_tab
otus_by_sample <- t(otus_by_sample)

# Remove phylogeny info
otus_by_sample <- otus_by_sample[-c(19:25), ]

# Look up sample names
otus_by_sample <- as.data.frame(otus_by_sample)
otus_by_sample$Sample <- rownames(otus_by_sample)
otus_by_sample <- otus_by_sample[,c(ncol(otus_by_sample),1:(ncol(otus_by_sample)-1))]
rownames(otus_by_sample) <- NULL
otus_by_sample$Sample <- meta[match(otus_by_sample$Sample,meta$Label),"Sample"]


# aggregate by sample type (https://stackoverflow.com/questions/1660124/how-to-sum-a-variable-by-group)
otu_sample_names <- otus_by_sample$Sample
indx <- sapply(otus_by_sample, is.factor)
otus_by_sample[indx] <- lapply(otus_by_sample[indx], function(x) as.numeric(as.character(x)))
otus_by_sample$Sample <- otu_sample_names

otus_agg_by_sample <- aggregate(. ~ Sample,otus_by_sample, "sum")


# Transform
otus_agg_by_sample <- t(otus_agg_by_sample)
colnames(otus_agg_by_sample) <- otus_agg_by_sample[1,]
otus_agg_by_sample <- otus_agg_by_sample[-1,]

# Filter OTUs by in bristles, not in sludge
otus_agg_by_sample <- as.data.frame(otus_agg_by_sample)
sludge_vs_bristles <- otus_agg_by_sample[otus_agg_by_sample$Sludge == 0 & otus_agg_by_sample$Bristles != 0,]
# sludge_vs_bristles[,-c(2,3,4)]

n_bristle_otus <- nrow(sludge_vs_bristles)
# Make histogram of bristle reads
ggplot(data=sludge_vs_bristles, aes(sludge_vs_bristles$Bristles)) + 
    geom_bar(col="black",fill="grey") +
    xlab("No. reads per OTU")+
    annotate("text",x=5,y=70,label=paste(n_bristle_otus, "OTUs found in bristles and not in sludge"))+
    theme_few()


```
Let's compare this data to the original biom file that didn't have singletons removed. How many of these OTUs came from singletons in the sludge, and how many did not show up at all in the sludge? Then let's make a bar chart of those two counts.
```{r Bristle OTU Reads from original biom file}
# Can't read in original file with read.biom because read.biom removes singletons then filters for only OTUs that had reads in my samples
    orig_biom=read.table("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/PICRUST/Exp15-microbiom COPY/otu_table.txt",header=T,sep="\t")

head(orig_biom)

# Sum bristle samples

meta$Label[c(which(meta$Sample=="Sludge"))] # Return label names which match sample "Sludge"
orig_biom$Orig_Sludge <- rowSums(orig_biom[,(meta$Label[c(which(meta$Sample=="Sludge"))])]) # Create a new summed column in the original biom file

# Match original sludge reads into new sludge vs bristles table by looking up OTU
sludge_vs_bristles$Orig_Sludge <- orig_biom$Orig_Sludge[match(rownames(sludge_vs_bristles),orig_biom$OTU.ID)]

# Plot
# Make histogram of read
ggplot(data=sludge_vs_bristles, aes(as.factor(sludge_vs_bristles$Orig_Sludge))) + 
    geom_bar(col="black",fill="grey") +
    xlab("No. reads in sludge per OTU without singleton removal")+
    geom_text(stat='count',aes(label=..count..),vjust=-1)+
  scale_y_continuous(limits=c(0,200))+
  theme_few()
```


These relationships are interesting. Many of the relationships look similar. Let's do a scatterplot matrix to see what these relationships look like.
```{r Scatterplot of ecological metrics}
# https://stackoverflow.com/questions/3735286/create-a-matrix-of-scatterplots-pairs-equivalent-in-ggplot2
ggpairs(S_merge, columns = c(2,3,9), aes(colour = Sample, alpha = 0.4))+theme_few()
```

Now that we've explored our ecological metrics, let's dig into some of the network analysis of our microbial communities.

# Microbial Community Networks

 So this is the basic works, but getting into some of the network development, what we are trying to extract is how our microbial community is interacting.
 There are several ways to produce co-occurrence networks.

 Pearson Correlations - generate linear relationship between OTU's. This means organisms have to increase and decrease at the same rate to become captured in the network. This can be of relative abundance, raw counts, or transformed data (generally centered-log-transformed)
 Spearman Correlations - generate rank-based relationships between OTU's. This allows data to increase or decrease together but isn't restricted to linear relationships. This again can be relative abundance, raw counts, or transformed data (generally centered-log-transformed).
 Other novel programs exist but will be outside the scope of this package
 SparCC
 Spiec-Easi (Speak-easy)
 Some people will use rarefied counts but this removes data and recent studies suggest NOT to do this.
 So here, we will use relative abundance to just get familiar with the ideas until a centered-log-ratio is implemented. The reason relative abundance isn't great for networks is that OTU's with large counts produce false correlations due to their predominance.


```{r Co-occurrence across all OTUs, message = FALSE, warning=FALSE}
  
  # This is an area of networks where we study OTU's in the same environment that "co-occur". First we will filter data by co-occurence. Minimum recommended is 20%.
  
  #filter OTU's to 50% presence in all samples
  biom_fil=cooccur_filter(RA=biom$RA.Otus,co_per=0.5)
  
  #run co-occurence. Taxon can be excluded and identified later if desired.
  biom_netw=cooccurrence(biom_fil,taxon = biom$taxon)
  
  #try plotting the data
  # plot.igraph(biom_netw$netw) # standard plot function for igraph package
  # tkplot(biom_netw$netw) #interactive plot viewer
  # rglplot(biom_netw$netw) #experimental 3d plot viewer
  
  #Or by using ggnet2 (https://briatte.github.io/ggnet/)
  packages(network)
  packages(sna)
  packages(RColorBrewer)
  packages(intergraph)
  
  # Visualize
  set.seed(101)
  ggnet2(biom_netw$netw, size = 6, color = "tomato")
  
```
What if we run the same code but only on the bristle and planktonic bacteria samples?
```{r Networks in Bristle and Planktonic Samples,fig.width=9,fig.height=9}
  
  # This is an area of networks where we study OTU's in the same environment that "co-occur". First we will filter data by co-occurence. Minimum recommended is 20%.
  
# Filter biom$RA.Otus to just bristle and final planktonic samples
biom$RA.Otus.bristles <- biom$RA.Otus[meta[meta$Sample=="Bristles"|meta$Sample=="Cycle 6","Label"],]
# Write to csv
write.csv(biom$RA.Otus.bristles,"biom_RA_OTus_bristles.csv")

  
#filter OTU's to 50% presence in all samples
  biom_fil=cooccur_filter(RA=biom$RA.Otus.bristles,co_per=0.5)
  
  #run co-occurence. Taxon can be excluded and identified later if desired.
  biom_netw=cooccurrence(data = biom_fil,taxon = biom$taxon,type="pearson",cor=.6,pval=.05)
 
  #try plotting the data
  # plot.igraph(biom_netw$netw) # standard plot function for igraph package
  # tkplot(biom_netw$netw) #interactive plot viewer
  # rglplot(biom_netw$netw) #experimental 3d plot viewer
  
  #Or by using ggnet2 (https://briatte.github.io/ggnet/)
  packages(network)
  packages(sna)
  packages(RColorBrewer)
  packages(intergraph)
  
  # Visualize
  set.seed(101)
  (network_plot <- ggnet2(biom_netw$netw, size = 6, color = "tomato", label=T, label.size=2))
 

```
# Community Detection
  Now once you've seen your network, you are probably noticing clusters together and maybe that's interesting to you. If so, community detection is next up. This will allow you to find interacting groups of OTU's that may be functioning together in your samples and worth investigating more thoroughly. 
  As always multiple methods exist and most can be found within the igraph package of R. Two review papers comparing community detection algorithms can be found here and here.
  To list a few algorithms:
  
  fastgreedy *greedy modularity maximisation
  infomap *information compression
  louvain *multilevel modularity maximization
  walktrap
  uses small random walks to identify most likely neighbors
  walktrap (modularity optimized - this package)
  same as walktrap but optimizes based on modularity
  Each one of these algorithms can utilize the network you just created to detect these community niches.
```{r plot community networks,fig.width=12,fig.height=9}
# infomap community detection. Try the different detection algorithms to understand how different your niches might be broken up
biom_info=infomap.community(biom_netw$netw)


#now add some color to your previous plot (Josh's Method)
  #plot(biom_netw$netw,vertex.color=as.factor(biom_info$membership))
  
#Create a custom color scale
n <- max(biom_info$membership)
library(RColorBrewer)
myColors <- colorRampPalette(brewer.pal(12, "Paired"))(n)
names(myColors) <- levels(as.factor(biom_info$membership))
colScale <- scale_colour_manual(name = "Cluster",values = myColors)

#Plot with ggnet
nodesize <- 8
set.seed(101)
(plot_ggnet <- ggnet2(biom_netw$netw, size = nodesize, 
                      node.color = as.factor(biom_info$membership), 
                      color.legend = "Cluster",
                      label=T, 
                      label.size=2)+
    colScale+
    guides(colour = guide_legend(override.aes = list(size=nodesize))))
    

```  
```{r}

  #and to understand how these subcommunities are present in your overall community
  #this may show a warning message if the community modules/clusters exceed 13. This is just because of lacking a distinct palette color for each cluster. It may also be harder to interpret yourself. You can plot anyway with the following line.
  plot_module=barplot_module(data=biom$RA.Otus,niche = biom_info,meta = meta, categories = "Sample")
  plot_module$plot+
    scale_x_discrete("Sample")+ # Labels categorical labels on x axis
    ylab("Relative Abundance (%)")+ 
    theme(legend.title=element_text(),legend.position="right",plot.title = element_text(hjust=0.5))+
    theme_few()
```



# Keystone Microbes
  
Next we'll try and find some keystone microbes (if there are some!). This is largely built on heuristics from modularity of bee pollination networks. Nevertheless it is the gold-standard at the moment. Keystone are identified as either:
 Hub: forms a dense set of network connections within its own module such that the disappearance of such OTU may signify large changes for module structure or module collapse
  Connector: Is largely connected to many different modules bringing together many different microbial niches. The disappearance of these OTU's may remove the ability of the niches to function together in the same environment
  
  This is passed our network information and module membership information. Because of an iterative process, this can sometimes take a little bit to work
```{r}
 biom_zipi=ZiPi(biom_netw$netw,modules=biom_info$membership)
```  
  
 
  
  this can be visualized as such. Sorry I haven't put a code for this yet but you get the table to plot with it as you will or export it to excel if you desire. 

```{r zi pi plot}
par(xpd=FALSE)
  plot(biom_zipi$P,biom_zipi$Z,
       ylim = c(-3.5,3),
       ylab="Zi",
       xlab="Pi")
  #connectors are defined as having a Pi value > 0.62
  abline(v=0.62)
  #hubs are defined as having a Zi value > 2.5
  abline(h=2.5)
  #we will color any hubs or connectors red
  points(biom_zipi$P[biom_zipi$P>=0.62],biom_zipi$Z[biom_zipi$P>=0.62],col="red",pch=1) # these cutoff values are from literature
  points(biom_zipi$P[biom_zipi$Z>=2.5],biom_zipi$Z[biom_zipi$Z>=2.5],col="red",pch=1) # these cutoff values are from literature
  
  
  # Alternatively in ggplot
  zi.pi.ggplot <- ggplot(biom_zipi, aes(x = P, y = Z, color = ifelse(P>=.62|Z>=2.5,"red","green"))) #these colors don't seem to be working
  zi.pi.ggplot <- zi.pi.ggplot + geom_point(size = 4) + theme_few() + xlab("Pi")+ ylab("Zi")+
  geom_vline(xintercept = 0.62)+
  geom_hline(yintercept = 2.5)+
  theme(legend.position="none") #turns legend off
  zi.pi.ggplot
  
  #Let's return those OTUs of interest
  packages(dplyr)
  biom_zipi_df <- as.data.frame(biom_zipi)
  filter(biom_zipi_df, P>=.62) # Connectors
  filter(biom_zipi_df, Z>=2.5) # Hubs
```


What sort of community network is this? Small world?

# KEGG Orthologs from PICRUST analysis
Here we want to answer three main questions. First, which types of genes were best enriched in each of our sample groups?
Second, were exoelectrogenic genes enriched in our bristles?
Third, which genes were enriched across each network cluster?

First, which types of genes were best enriched in each of our sample groups?
```{r which KOs were best enriched in each of our sample groups, fig.width=9,fig.height=9}
# Let's try level 2 - it's got good granularity without too many different types of KOs
l2_picrust <- read.biom(
  "C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/PICRUST/Exp15-microbiom COPY/predicted_metagenome.L2.txt",
  new = T, metagenome = T)


# Align my data using label names in metadata
meta$Label
rownames(l2_picrust$RA.genes) # see if row names match up in same order
l2_picrust$RA.genes <- l2_picrust$RA.genes[match(meta$Label,rownames(l2_picrust$RA.genes)),] # make samples line up according to meta data so we can aggregate

# aggregate by sample type (https://stackoverflow.com/questions/1660124/how-to-sum-a-variable-by-group)
l2_agg = aggregate(l2_picrust$RA.genes,by=list(meta$Sample),FUN=mean)

# Then melt
melt_l2_agg <- melt(l2_agg,id="Group.1")

# Generate heat map
ggplot(data=melt_l2_agg,aes(x=Group.1,y=variable,fill=value))+
  geom_tile(color="white",size=.8)+ # puts padding in between colored cells
  scale_fill_gradient2("Gene relative abundance (%)\n",guide="colourbar",high="steelblue",low="white")+
  xlab("Sample")+
  ylab("KEGG Ortholog Group")+
  scale_x_discrete(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0, 0)) +
  theme_few()+
  theme(legend.title = element_text(hjust = 5))+
  ylim(rev(levels(melt_l2_agg$variable))) # Puts alphabetically from top

# What if we rescale and plot? https://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/
melt_l2_agg <- ddply(melt_l2_agg, .(variable), transform, rescale = scales::rescale(value)) # this rescales each value 0:1 by variable

(l2_heatmap1 <- ggplot(data=melt_l2_agg,aes(x=Group.1,y=variable,fill=rescale))+
  geom_tile(color="white",size=.8)+ # puts padding in between colored cells
  scale_fill_gradient2("Rescaled gene relative abundance\n",guide="colourbar",high="steelblue",low="white")+
  xlab("Sample")+
  ylab("KEGG Ortholog Group")+
  scale_x_discrete(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0, 0)) +
  theme_few()+
  theme(legend.title = element_text(hjust = 5))+
  ylim(rev(levels(melt_l2_agg$variable)))) # Puts alphabetically from top
  # ggtitle("Gene abundance over time")

# What if we scale and plot? https://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/
melt_l2_agg <- ddply(melt_l2_agg, .(variable), transform, scale = scale(value)) # this rescales each value 0:1 by variable

(l2_heatmap1 <- ggplot(data=melt_l2_agg,aes(x=Group.1,y=variable,fill=scale))+
  geom_tile(color="white",size=.8)+ # puts padding in between colored cells
  scale_fill_gradient2("Scaled gene relative abundance\n",guide="colourbar",high="steelblue",low="white")+
  xlab("Sample")+
  ylab("KEGG Ortholog Group")+
  scale_x_discrete(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0, 0)) +
  theme_few()+
  theme(legend.title = element_text(hjust = 5))+
  ylim(rev(levels(melt_l2_agg$variable)))) # Puts alphabetically from top
  # ggtitle("Gene abundance over time")
```
Second, were exoelectrogenic genes enriched in our bristles?
```{r were exoelectrogenic genes enriched in our bristles}
# Get data from PICRUSt JSON file                
predicted_metagenome = read.biom("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/PICRUST/Exp15-microbiom COPY/PICRUST Metagenome USE THIS.tab",new=T,metagenome=T)
# predicted_metagenome$RA.genes[1:10,1:10] # View data # Note this gives relative abundance in percent from 0 to 100

# Modify data to align it               -
# meta$Label # uses metadata from the fir data set of Josh's microbiome package
# rownames(predicted_metagenome$RA.genes) # see if row names match up in same order
predicted_metagenome$RA.genes <- predicted_metagenome$RA.genes[match(meta$Label,rownames(predicted_metagenome$RA.genes)),]
# rownames(predicted_metagenome$RA.genes)

# Aggregate genes by sample  
genes_agg = aggregate(predicted_metagenome$RA.genes,by=list(meta$Sample),FUN=mean)


# Convert wide format into long format table
genes_melt=melt(genes_agg,id="Group.1")

# Filter by K01179 (Endoglucanase) and K05350 and K00430 (or whatever genes of interest)

kos_exoelectrogenic <- read.csv("kos_exoelectrogenic.csv",header=FALSE,fileEncoding="UTF-8-BOM")


genes_filtered <- genes_melt[which(genes_melt$variable %in% unlist(kos_exoelectrogenic)),]


# Make heat map

ggplot(data=genes_filtered,aes(x=Group.1,y=variable,fill=value))+
  geom_tile()+
  scale_fill_gradient2("Gene Relative Abundance",guide="colourbar",high="#FF5733",low="#FDFEFE",midpoint=mean(genes_filtered$value)-.02)+
  xlab("Timepoint")+
  ylab("KEGG Ortholog")+
  ggtitle("Gene abundance over time")

# Now what if we rescale the data?
genes_filtered <- ddply(genes_filtered, .(variable), transform, rescale = scales::rescale(value)) # this rescales each value 0:1 by variable

ggplot(data=genes_filtered,aes(x=Group.1,y=variable,fill=rescale))+
  geom_tile()+
  scale_fill_gradient2("Scaled gene relative abundance \n",guide="colourbar",high="#FF5733",low="#FDFEFE")+
  xlab("Sample")+
  ylab("KEGG Ortholog")

```
What if we run anova on these scaled gene relative abundances by sample? Fit rescale to Group.1
```{r anova on scaled KO relative abundances by sample}
genes_filtered$logrescale <- log10(genes_filtered$rescale)
ggplot(data=genes_filtered, aes(x=factor(Group.1,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = rescale)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Scaled gene relative abundance") +
    theme_few()
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(rescale ~ Group.1, data = genes_filtered)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model.genes.filtered = anova(model)
  anova.model.genes.filtered
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_kos = aov(rescale~Group.1,data=genes_filtered)  #do the analysis of variance
  # cat("\n")
  # summary(aov_diversity)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_kos,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results  
  pvaluecolumn <- anova.model.genes.filtered$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups
  cat("\n")
  print(TukeyHSD(aov_kos))
```
Can we show these differences another way by doing LDA on them?
```{r LDA on KOs by sample}
# https://www.r-bloggers.com/computing-and-visualizing-lda-in-r/

# Get data
#predicted_metagenome$RA.genes

# Convert wide format into long format table
genes_melt_LDA <- melt(as.matrix(predicted_metagenome$RA.genes))
# head(genes_melt_LDA)

# Filter by exoelectrogenic KOs
genes_filtered_LDA <- genes_melt_LDA[which(genes_melt_LDA$X2 %in% unlist(kos_exoelectrogenic)),]
# head(genes_filtered_LDA)

# Unmelt
genes_filtered_LDA <- dcast(data=genes_filtered_LDA,formula = X1~X2)

# tag on sample type
genes_filtered_LDA$Sample <- meta[match(genes_filtered_LDA$X1,meta$Label),"Sample"]
# head(genes_filtered_LDA)
# output for JMP
# write.csv(genes_filtered_LDA,"genes_filtered_LDA.csv")

# Run LDA
packages(MASS)
lda <- lda(formula = Sample ~ ., 
         data = genes_filtered_LDA[,c(2:ncol(genes_filtered_LDA))] 
         )
lda

# Get significance
prop.lda = lda$svd^2/sum(lda$svd^2)

# Plot
plda <- predict(object = lda,
                newdata = genes_filtered_LDA[,c(2:ncol(genes_filtered_LDA))])

dataset = data.frame(Sample = genes_filtered_LDA[,"Sample"],
                     lda = plda$x)

(p1 <- ggplot(dataset) + geom_point(aes(lda.LD1, lda.LD2, colour = Sample, shape = Sample), size = 2.5) + 
  labs(x = paste("LD1 (", percent(prop.lda[1]), ")", sep=""),
       y = paste("LD2 (", percent(prop.lda[2]), ")", sep="")) +
  theme_few()
  )

# Wilks Lambda
dep.vars <- cbind(genes_filtered_LDA[,c(2:8)])
lda_fit <- manova(as.matrix(dep.vars) ~ genes_filtered_LDA$Sample)
summary(lda_fit, test="Wilks")
```
Now let's try preprocessing our numerical variables with Box Cox, centering, and scaling.
```{r LDA on KOs by sample with preprocessing, message=FALSE, warning=FALSE}
# Preprocessing
# Visualize
multiplehist <- melt(genes_filtered_LDA[,c(2:ncol(genes_filtered_LDA))])
ggplot(multiplehist,aes(x = value)) + 
    facet_wrap(~variable,scales = "free") + 
    geom_histogram(col="black", 
                 fill="grey") +
    theme_few()
# Transformation
packages(caret)
packages(e1071)
trans = preProcess(genes_filtered_LDA[,c(2:ncol(genes_filtered_LDA)-1)],
                   c("BoxCox", "center", "scale"))
kos_trans = data.frame(trans = predict(trans, genes_filtered_LDA[,c(2:ncol(genes_filtered_LDA)-1)]))
# Visualize again
multiplehist <- melt(kos_trans)
ggplot(multiplehist,aes(x = value)) + 
    facet_wrap(~variable,scales = "free") + 
    geom_histogram(col="black", 
                 fill="grey") +
    theme_few()

# Attach sample
kos_trans$Sample <- genes_filtered_LDA$Sample
genes_filtered_LDA_trans <- kos_trans

# Run LDA
packages(MASS)
lda <- lda(formula = Sample ~ ., 
         data = genes_filtered_LDA_trans[,c(2:ncol(genes_filtered_LDA))] 
         )
lda

# Get significance
prop.lda = lda$svd^2/sum(lda$svd^2)

# Plot
plda <- predict(object = lda,
                newdata = genes_filtered_LDA_trans[,c(2:ncol(genes_filtered_LDA))])

dataset = data.frame(Sample = genes_filtered_LDA_trans[,"Sample"],
                     lda = plda$x)

(p1 <- ggplot(dataset) + geom_point(aes(lda.LD1, lda.LD2, colour = Sample, shape = Sample), size = 2.5) + 
  labs(x = paste("LD1 (", percent(prop.lda[1]), ")", sep=""),
       y = paste("LD2 (", percent(prop.lda[2]), ")", sep="")) +
  theme_few()
  )

# Wilks Lambda
dep.vars <- cbind(genes_filtered_LDA_trans[,c(2:8)])
lda_fit <- manova(as.matrix(dep.vars) ~ genes_filtered_LDA_trans$Sample)
summary(lda_fit, test="Wilks")
summary(aov(as.matrix(dep.vars) ~ genes_filtered_LDA_trans$Sample))

# Put biplot on LDA?
# https://stats.stackexchange.com/questions/82497/can-the-scaling-values-in-a-linear-discriminant-analysis-lda-be-used-to-plot-e
packages(devtools)
install_github('fawda123/ggord')
library(ggord)

ord <- lda

p <- ggord(ord, genes_filtered_LDA_trans$Sample)
p #Note: needs to be cleaned up. Legend is messed up: shows two groupings.

```
What if we run anova on these Box Cox transformed relative abundances by sample? Fit rescale to Group.1
```{r anova on the Box Cox transformed relative abundances by sample}
# Data
data <- melt(genes_filtered_LDA_trans[,-1])


ggplot(data, aes(x=factor(Sample,levels = c("Sludge","Cycle 3","Cycle 6","Bristles","Cathode")), y = value)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample Group") +
    ylab("Transformed exoelectrogenic 
         KO relative abundance") +
    theme_few()
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(value ~ Sample, data)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model.genes.filtered = anova(model)
  anova.model.genes.filtered
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_kos = aov(value ~ Sample, data)  #do the analysis of variance
  # cat("\n")
  # summary(aov_diversity)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_kos,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results
  pvaluecolumn <- anova.model.genes.filtered$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups
  cat("\n")
  print(TukeyHSD(aov_kos))
```

Third, which genes were enriched across each network cluster?

```{r aggregate KOs by network cluster}
# Picrust table (takes 2 min to read)
a <- proc.time()
ko_metagenome_contributions <- read.table("C:/Users/Ryan/Google Drive/UC Davis/Publications/MFC Microbiome Paper - Experiment 15/PICRUST/Exp15-microbiom COPY/ko_metagenome_contributions.tab", header = T, sep = "\t", fill=TRUE)
proc.time()-a
# chop off unnecessary phylogeny
ko_metagenome_contributions <- ko_metagenome_contributions[,-c(9:15)]
# which quantitative column is of interest? GeneCountPerGenome because it's the only one that doesn't involve relative abundance normalized to sample

# create lookup table
otu_cluster_lookup <- data.frame("OTU" = biom_info$names,"Cluster" = biom_info$membership)
# Lookup cluster number from lookup table
ko_metagenome_contributions$Cluster <- otu_cluster_lookup[match(ko_metagenome_contributions$OTU,otu_cluster_lookup$OTU),"Cluster"]
ko_metagenome_contributions[1:10,]

#filter out NA clusters
filt_ko_metagenome_contributions <- ko_metagenome_contributions[!is.na(ko_metagenome_contributions$Cluster),]
#remove unnecessary columns
filt_ko_metagenome_contributions <- filt_ko_metagenome_contributions[,-c(5:8)]
head(filt_ko_metagenome_contributions)

# within each cluster, sum GeneCountPerGenome by Gene
filt_ko_metagenome_contributions <- aggregate(GeneCountPerGenome~Gene+Cluster,filt_ko_metagenome_contributions,sum)
head(filt_ko_metagenome_contributions)

# calc relative abundance of KOs within total KO GeneCountPerGenome in cluster
require(dplyr)
filt_ko_metagenome_contributions <- filt_ko_metagenome_contributions %>% group_by(Cluster) %>% mutate(relAbundByCluster = GeneCountPerGenome / sum(GeneCountPerGenome))

#Test it worked (yes it did)
filt_ko_metagenome_contributions[1,]
aggregate(GeneCountPerGenome~Cluster,filt_ko_metagenome_contributions,sum)[1,]


# filter by target KOs
filt_ko_metagenome_contributions <- filt_ko_metagenome_contributions[which(filt_ko_metagenome_contributions$Gene %in% unlist(kos_exoelectrogenic)),]
head(filt_ko_metagenome_contributions)

# tidy up names
colnames(filt_ko_metagenome_contributions) <- c("Gene","Cluster","GeneCount","relAbund")

# Rescale 0 to 1 for heat map
filt_ko_metagenome_contributions <- ddply(filt_ko_metagenome_contributions, .(Gene), transform, rescale = scales::rescale(relAbund))

# plot heat map
ggplot(data=filt_ko_metagenome_contributions,aes(x=factor(Cluster),y=Gene,fill=rescale))+
  geom_tile()+
  scale_fill_gradient2("Rescaled gene relative abundance \n",guide="colourbar",high="#FF5733",low="#FDFEFE")+
  xlab("Cluster")+
  ylab("KEGG Ortholog")+
  theme_few()+
 scale_x_discrete(expand = c(0,0))+
 scale_y_discrete(expand = c(0,0)) +
 coord_fixed(ratio=1)
```

```{r}
# Box cox, center, scale across KOs for all clusters
# Preprocessing
# Visualize
ggplot(filt_ko_metagenome_contributions,aes(x = relAbund)) + 
    facet_wrap(~Gene,scales = "free") + 
    geom_histogram(col="black", 
                 fill="grey") +
    theme_few()
# Transformation
# unstack data
data1 <- melt(filt_ko_metagenome_contributions[,c(1,2,4)], id.vars = c("Gene", "Cluster"))
data1 <- dcast(data1,Cluster~Gene)
# fill NA with zero
data1[is.na(data1)] <- 0
#preprocess
packages(caret)
packages(e1071)
trans = preProcess(data1[,c(2:ncol(data1))], c("BoxCox", "center", "scale"))
cluster_kos_trans = data.frame(trans = predict(trans, data1[,c(2:ncol(data1))]))
# Visualize again
multiplehist <- melt(cluster_kos_trans)
ggplot(multiplehist,aes(x = value)) + 
    facet_wrap(~variable,scales = "free") + 
    geom_histogram(col="black", 
                 fill="grey") +
    theme_few()

# Attach cluster number to transformed variables
cluster_kos_trans$Cluster <- data1$Cluster

```
ANOVA on KOs in Clusters
```{r ANOVA on KOs in Clusters}
# Format data
cluster_kos_trans_format <- cluster_kos_trans[c(ncol(cluster_kos_trans),1:ncol(cluster_kos_trans)-1)]
data <- melt(cluster_kos_trans_format,id="Cluster")
data[1] <- factor(unlist(data[1]))
ggplot(data, aes(x=Cluster, y = value)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Cluster") +
    ylab("Rescaled KO relative abundance") +
    theme_few()
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(value ~ Cluster, data = data)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  # The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals
  # confint(model)
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_kos = aov(value~Cluster,data=data)  #do the analysis of variance
  # cat("\n")
  # summary(aov_diversity)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_kos,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results  
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p Value column
  pvalue = pvaluecolumn[1] # Extracts first row from p value column
  cat("\n")
  cat("ANOVA p Value is",pvalue,"\n") # Displays p Value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups
  cat("\n")
 
Tukey <- TukeyHSD(aov_kos)
Tukey$Cluster[Tukey$Cluster[,4]<.05,]

```
Interesting. Why do clusters 13 and 17 have so many exoelectrogenic genes? Let's make a bar chart of clusters 13 and 17 across our samples.
```{r bar chart of clusters 13 and 17 across samples}
#assign clusters of interest
clusofinterest <- c(13,17)
# make a long form table we can aggregate
ix <- which(colnames(biom$biom_tab) %in% c("Kingdom","Phylum","Class","Order","Family","Genus","Species")) # remove taxonomy info
data <- biom$biom_tab[,-ix] 
data <- t(data)
data <- melt(data)
colnames(data) <- c("Label","OTU","Reads")
#assign clusters to OTUs
data$Cluster <- otu_cluster_lookup[match(data$OTU,otu_cluster_lookup$OTU),"Cluster"]
# aggregate reads by label and cluster
agg_data <- aggregate(data$Reads,
    by = list(Label = data$Label, Cluster = data$Cluster),
    FUN = function(x) c(sum = sum(x)))
colnames(agg_data) <- c("Label","Cluster","Cluster_reads_in_label")
# aggregate reads by label
agg_data_label <- aggregate(data$Reads,
    by = list(Label = data$Label),
    FUN = function(x) c(sum = sum(x)))
# attach to agg_data
agg_data$label_reads <- agg_data_label[match(agg_data$Label,agg_data_label$Label),"x"]
# RA
agg_data$RA_cluster_in_label <- 100*agg_data$Cluster_reads_in_label/agg_data$label_reads
#assign samples to labels
agg_data$Sample <- meta[match(agg_data$Label,meta$Label),"Sample"]
#filter for clusters of interest
filter_agg_data <- agg_data[agg_data$Cluster %in% clusofinterest,]

# get data in shape for plotting https://www.r-bloggers.com/building-barplots-with-error-bars/
myData <- aggregate(filter_agg_data$RA_cluster_in_label,
    by = list(Sample = filter_agg_data$Sample, Cluster = filter_agg_data$Cluster),
    FUN = function(x) c(mean = mean(x), sd = sd(x),
                        n = length(x)))

myData <- do.call(data.frame, myData)
myData$se <- myData$x.sd / sqrt(myData$x.n)

colnames(myData) <- c("Sample", "Cluster", "mean", "sd", "n", "se")

myData$names <- c(paste(myData$Sample,"/ Cluster",
                        myData$Cluster))
#Plot 
limits <- aes(ymax = myData$mean + myData$sd,
              ymin = myData$mean - myData$sd)

p <- ggplot(data = myData, aes(x = factor(Sample), y = mean,
               fill = factor(Cluster)))

p + geom_bar(stat = "identity",
             position = position_dodge(0.9)) +
  geom_errorbar(limits, position = position_dodge(0.9),
                width = 0.25) +
  labs(x = "Sample", y = "Relative Abundance (%)") +
  scale_fill_discrete(name = "Cluster")+
  theme_few()
```

```{r anova on cluster 13 RA across samples}
data <- filter_agg_data[filter_agg_data$Cluster %in% 13,]

ggplot(data, aes(x=Sample, y = RA_cluster_in_label)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample") +
    ylab("Cluster 13 Relative Abundance (%)") +
    theme_few()
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(RA_cluster_in_label ~ Sample, data = data)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_kos = aov(RA_cluster_in_label~Sample,data=data)  #do the analysis of variance
  cat("\n")
  summary(aov_kos)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_kos,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results  
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p RA_cluster_in_label column
  pvalue = pvaluecolumn[1] # Extracts first row from p RA_cluster_in_label column
  cat("\n")
  cat("ANOVA p value is",pvalue,"\n") # Displays p value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups
  cat("\n")
 
TukeyHSD(aov_kos)
```
```{r anova on cluster 17 RA across samples}
data <- filter_agg_data[filter_agg_data$Cluster %in% 17,]

ggplot(data, aes(x=Sample, y = RA_cluster_in_label)) +
    geom_boxplot(fill = "grey80", colour = "blue") +
    scale_x_discrete() + xlab("Sample") +
    ylab("Cluster 17 Relative Abundance (%)") +
    theme_few()
  
# Fit a model using the lm function and look at the parameter estimates and standard errors for the treatment effects
  # https://www.r-bloggers.com/one-way-analysis-of-variance-anova/
  model = lm(RA_cluster_in_label ~ Sample, data = data)
  # We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. 
  # summary(model)
  # An analysis of variance table for this model can be produced via the anova command
  anova.model = anova(model)
  anova.model
  
  
# Alternative method (http://personality-project.org/r/r.guide/r.anova.html)
  aov_kos = aov(RA_cluster_in_label~Sample,data=data)  #do the analysis of variance
  cat("\n")
  summary(aov_kos)                                    #show the summary table
  cat("\n")
  print(model.tables(aov_kos,"means"),digits=3)       #report the means and the number of subjects/cell
  
# Interpret the Omnibus ANOVA Test Results  
  pvaluecolumn <- anova.model$"Pr(>F)" # Extracts p RA_cluster_in_label column
  pvalue = pvaluecolumn[1] # Extracts first row from p RA_cluster_in_label column
  cat("\n")
  cat("ANOVA p value is",pvalue,"\n") # Displays p value
  if (pvalue>.05) {
    writeLines("Results are not significant. Fail to reject null hypothesis.")
  } else {
    writeLines("Results are significant. Reject null hypothesis.")
  }

# Run Post-Hoc Tukey HSD between treatment groups
  cat("\n")
 
TukeyHSD(aov_kos)
```
```{r pie chart of Cluster 13 filled with OTUs}
# https://stackoverflow.com/questions/42489277/multiple-ggplot-pie-charts-with-whole-pies
# make a long form table we can aggregate
ix <- which(colnames(biom$biom_tab) %in% c("Kingdom","Phylum","Class","Order","Family","Genus","Species")) # remove taxonomy info
data <- biom$biom_tab[,-ix] 
data <- t(data)
data <- melt(data)
colnames(data) <- c("Label","OTU","Reads")
#assign clusters to OTUs
data$Cluster <- otu_cluster_lookup[match(data$OTU,otu_cluster_lookup$OTU),"Cluster"]
# assign samples to labels
data$Sample <- meta[match(data$Label,meta$Label),"Sample"]
# filter for bristles and cluster 13
data <- data[data$Sample=="Bristles" & data$Cluster %in% 13,]
data$OTU <- as.factor(data$OTU)
# plot
ggplot(data = data, aes(x = "", y = Reads, fill = OTU )) + 
    geom_bar(stat = "identity", position = position_fill()) +
    geom_text(aes(label = Reads), position = position_fill(vjust = 0.2)) +
    coord_polar(theta = "y") +
    facet_wrap(~ Label)  +
    theme(axis.title.x = element_blank(),
    axis.title.y = element_blank()) + theme(legend.position='right') + guides(fill=guide_legend(nrow=3,byrow=TRUE))+
    ggtitle("Cluster 13 OTUs in Bristles")
```
What taxonomies are associated with these OTUs?
```{r}
biom$taxon[biom$taxon$otus %in% data$OTU,]
```

# Yet to be done: Community Connects to the output
Here we will use ModuleEigengenes from the WGCNA package in R. WGCNA will require several installations from the Bioconductor site. This was developed to link genetic studies to diseases and conditions in the medical field. While WGCNA has their own community detection algorith embedded, this wrapper allows us to substitute our own and establish correlation between our detected communities and variables we've recored. The output is ready to be plotted with ggplot2.
```{r}
  # #first we call the function to establish the relationships. 
  # biom_eigen=eigen_correlation(biom$RA.Otus[-c(5,8),],community = biom_info,metadata = meta[-(1:2),],categories = c("Xylanase.IU.g.dry.matter","Endoglucanase.IU..g.dry.matter","cCER"))
  # 
  # #from there we can plot these correlations in a heatmap to visualize the relationship. 
  # ggplot(data=biom_eigen$melt_cor,aes(x=as.factor(variable), y=category,fill=value))+
  #   geom_tile(colour="#B8B8B8")+
  #   #geom_text(aes(label=value))+
  #   scale_fill_gradient2("Degreee of \n Correlation",guide = "colourbar",high = "#7DEB5F",mid="#F0EE54",low="#F3633F",na.value="white",limits=c(-0.75,0.75))+ 
  #   ylab("")+
  #   xlab("Cluster/Module")+
  #   labs(fill="Cluster to Deconstruction")+
  #   scale_y_discrete(labels=c("Xylanase","Endoglucanase","cCER"))
  # 
  # #we should probably filter on significance though
  # ggplot(data=biom_eigen$melt_cor,aes(x=as.factor(variable), y=category,fill=ifelse(pval<=0.1,value,NA)))+
  #   geom_tile(colour="#B8B8B8")+
  #   #geom_text(aes(label=value))+
  #   scale_fill_gradient2("Degreee of \n Correlation",guide = "colourbar",high = "#7DEB5F",mid="#F0EE54",low="#F3633F",na.value="white",limits=c(-0.75,0.75))+ 
  #   ylab("")+
  #   xlab("Cluster/Module")+
  #   labs(fill="Cluster to Deconstruction")+
  #   scale_y_discrete(labels=c("Xylanase","Endoglucanase","cCER"))
```


